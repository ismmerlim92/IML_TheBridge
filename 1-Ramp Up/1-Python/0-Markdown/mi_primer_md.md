# Práctica Transformers para LLM

- [Práctica Transformers para LLM](#práctica-transformers-para-llm)
  - [Acerca del proyecto](#acerca-del-proyecto)
  - [Next Steps](#next-steps)



Contribuyentes:

- Xiao Ching
- Shrinivasa Thakur
- Joey Kelly
- Kevin Gomes
- [Alberto Becerra](https://www.linkedin.com/in/alberto-becerra-tome-data-science-instructor/)

## Acerca del proyecto

En los últimos años, ha habido grandes avances en el mundo de la **Inteligencia Artificial**. La aparición de grandes modelos de la IA como *GPT-3*, *Stable Difussion* o *Whisper* han revolucionado el contexto del Data Science. El principal factor que ha hecho que esto ocurra ha sido la introducción de los **Transformer**. En este proyecto mostramos varios ejemplos y comparaciones entre el rendimiento de modelos entrenados con y sin la tecnología [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).

$$
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$


![GPT3](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)

## Next Steps